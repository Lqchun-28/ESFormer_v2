# æ–‡ä»¶è·¯å¾„: test_esformer_fixed.py
# ä¿®å¤ç‰ˆæœ¬çš„æµ‹è¯•è„šæœ¬

import torch
import torch.nn as nn
import numpy as np
from easydict import EasyDict
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/workspace/OpenPCDet')


def test_window_attention():
    """æµ‹è¯• WindowAttention æ¨¡å—"""
    print("æµ‹è¯• WindowAttention...")
    
    try:
        from pcdet.models.backbones_2d.swin_transformer_plus import WindowAttention
        
        dim = 96
        window_size = (7, 7)
        num_heads = 3
        
        attn = WindowAttention(dim, window_size, num_heads)
        
        # æµ‹è¯•è¾“å…¥
        B_, N, C = 4, 49, 96  # 4ä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£7x7=49ä¸ªä½ç½®ï¼Œ96ä¸ªé€šé“
        x = torch.randn(B_, N, C)
        
        with torch.no_grad():
            output = attn(x)
        
        assert output.shape == x.shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {x.shape}"
        print(f"WindowAttention è¾“å…¥/è¾“å‡ºå½¢çŠ¶: {x.shape}")
        print("âœ… WindowAttention æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ WindowAttention æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_swin_transformer_block():
    """æµ‹è¯• SwinTransformerBlock"""
    print("æµ‹è¯• SwinTransformerBlock...")
    
    try:
        from pcdet.models.backbones_2d.swin_transformer_plus import SwinTransformerBlock
        
        dim = 96
        num_heads = 3
        window_size = 7
        
        block = SwinTransformerBlock(dim, num_heads, window_size)
        
        # è®¾ç½®ç‰¹å¾å›¾å°ºå¯¸ - ä½¿ç”¨èƒ½è¢«çª—å£å¤§å°æ•´é™¤çš„å°ºå¯¸
        H, W = 56, 56  # 56 = 7 * 8, èƒ½è¢«çª—å£å¤§å°7æ•´é™¤
        block.H = H
        block.W = W
        
        # æµ‹è¯•è¾“å…¥
        B, L, C = 2, H * W, dim
        x = torch.randn(B, L, C)
        
        with torch.no_grad():
            output = block(x)
        
        assert output.shape == x.shape, f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {output.shape} vs {x.shape}"
        print(f"SwinTransformerBlock è¾“å…¥/è¾“å‡ºå½¢çŠ¶: {x.shape}")
        print("âœ… SwinTransformerBlock æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ SwinTransformerBlock æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_patch_merging():
    """æµ‹è¯• PatchMerging"""
    print("æµ‹è¯• PatchMerging...")
    
    try:
        from pcdet.models.backbones_2d.swin_transformer_plus import PatchMerging
        
        dim = 96
        merging = PatchMerging(dim)
        
        # æµ‹è¯•è¾“å…¥ - ä½¿ç”¨å¶æ•°å°ºå¯¸
        H, W = 56, 56
        B, L, C = 2, H * W, dim
        x = torch.randn(B, L, C)
        
        with torch.no_grad():
            x_merged, sparse_mask = merging(x, H, W)
        
        expected_L = (H // 2) * (W // 2)
        expected_C = 2 * C
        
        print(f"è¾“å…¥: {x.shape}, H={H}, W={W}")
        print(f"è¾“å‡º: {x_merged.shape}")
        print(f"æœŸæœ›è¾“å‡º: [{B}, {expected_L}, {expected_C}]")
        
        assert x_merged.shape == (B, expected_L, expected_C), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {x_merged.shape}"
        
        print("âœ… PatchMerging æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ PatchMerging æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_basic_layer():
    """æµ‹è¯• BasicLayer"""
    print("æµ‹è¯• BasicLayer...")
    
    try:
        from pcdet.models.backbones_2d.swin_transformer_plus import BasicLayer, PatchMerging
        
        dim = 96
        depth = 2
        num_heads = 3
        window_size = 7
        
        # æµ‹è¯•æœ‰ä¸‹é‡‡æ ·çš„å±‚
        layer = BasicLayer(
            dim=dim,
            depth=depth,
            num_heads=num_heads,
            window_size=window_size,
            downsample=PatchMerging
        )
        
        # æµ‹è¯•è¾“å…¥ - ä½¿ç”¨èƒ½è¢«çª—å£å¤§å°æ•´é™¤çš„å°ºå¯¸
        H, W = 56, 56
        B, L, C = 2, H * W, dim
        x = torch.randn(B, L, C)
        
        with torch.no_grad():
            result = layer(x, H, W)
        
        # è§£åŒ…è¿”å›žå€¼
        x_out, H_out, W_out, x_down, H_down, W_down, sparse_mask = result
        
        print(f"è¾“å…¥: {x.shape}, H={H}, W={W}")
        print(f"å½“å‰å±‚è¾“å‡º: {x_out.shape}, H={H_out}, W={W_out}")
        print(f"ä¸‹é‡‡æ ·è¾“å‡º: {x_down.shape}, H={H_down}, W={W_down}")
        
        # éªŒè¯ç»´åº¦ - ä¿®æ­£æµ‹è¯•é€»è¾‘
        assert x_out.shape == (B, C, H, W), f"å½“å‰å±‚è¾“å‡ºå½¢çŠ¶é”™è¯¯: {x_out.shape}"
        assert H_down == H // 2 and W_down == W // 2, f"ä¸‹é‡‡æ ·å°ºå¯¸é”™è¯¯: {H_down}, {W_down}"
        # ä¿®æ­£ï¼šx_down.shape[1] æ˜¯åºåˆ—é•¿åº¦ï¼Œä¸æ˜¯é€šé“æ•°
        expected_seq_len = H_down * W_down
        assert x_down.shape[1] == expected_seq_len, f"ä¸‹é‡‡æ ·åºåˆ—é•¿åº¦é”™è¯¯: {x_down.shape[1]} vs {expected_seq_len}"
        # é€šé“æ•°åº”è¯¥æ˜¯ x_down.shape[2]
        assert x_down.shape[2] == 2 * C, f"ä¸‹é‡‡æ ·é€šé“æ•°é”™è¯¯: {x_down.shape[2]} vs {2*C}"
        
        print("âœ… BasicLayer æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ BasicLayer æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_swin_transformer_plus():
    """æµ‹è¯• SwinTransformerPlus backbone"""
    print("æµ‹è¯• SwinTransformerPlus...")
    
    try:
        from pcdet.models.backbones_2d.swin_transformer_plus import SwinTransformerPlus
        
        # æ¨¡æ‹Ÿé…ç½®
        model_cfg = EasyDict({
            'PATCH_SIZE': [4, 4],
            'EMBED_DIM': 96,
            'DEPTHS': [2, 2, 6, 2],
            'NUM_HEADS': [3, 6, 12, 24],
            'WINDOW_SIZE': 7,
            'MLP_RATIO': 4.0,
            'QKV_BIAS': True,
            'DROP_RATE': 0.0,
            'ATTN_DROP_RATE': 0.0,
            'DROP_PATH_RATE': 0.1
        })
        
        input_channels = 64
        
        # åˆ›å»ºæ¨¡åž‹
        model = SwinTransformerPlus(model_cfg, input_channels)
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ - ç¡®ä¿å°ºå¯¸èƒ½è¢«patch_sizeæ•´é™¤ï¼Œä¸”patchåŽèƒ½è¢«window_sizeæ•´é™¤
        batch_size = 2
        # 180 / 4 = 45, ä½†45ä¸èƒ½è¢«7æ•´é™¤ï¼Œæ”¹ç”¨æ›´åˆé€‚çš„å°ºå¯¸
        # ä½¿ç”¨ 224: 224/4=56, 56èƒ½è¢«7æ•´é™¤
        H, W = 224, 224
        
        batch_dict = {
            'spatial_features': torch.randn(batch_size, input_channels, H, W)
        }
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output_dict = model(batch_dict)
        
        # æ£€æŸ¥è¾“å‡º
        assert 'spatial_features_2d' in output_dict
        output_features = output_dict['spatial_features_2d']
        
        print(f"è¾“å…¥ç‰¹å¾å›¾å°ºå¯¸: {batch_dict['spatial_features'].shape}")
        print(f"è¾“å‡ºç‰¹å¾å›¾å°ºå¯¸: {output_features.shape}")
        print(f"æœŸæœ›è¾“å‡ºé€šé“æ•°: {model.num_bev_features}")
        print(f"å®žé™…è¾“å‡ºé€šé“æ•°: {output_features.shape[1]}")
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        expected_channels = model.num_bev_features
        assert output_features.shape[1] == expected_channels, f"è¾“å‡ºé€šé“æ•°ä¸åŒ¹é…: {output_features.shape[1]} vs {expected_channels}"
        
        print("âœ… SwinTransformerPlus æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ SwinTransformerPlus æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_esformer_integration():
    """æµ‹è¯• ESFormer å®Œæ•´é›†æˆ"""
    print("æµ‹è¯• ESFormer å®Œæ•´é›†æˆ...")
    
    try:
        # è¿™ä¸ªæµ‹è¯•éœ€è¦å®Œæ•´çš„æ•°æ®é›†ç±»ï¼Œæš‚æ—¶è·³è¿‡
        # ä½†å¯ä»¥æµ‹è¯•æ¨¡åž‹åˆ›å»º
        print("âš ï¸ ESFormer é›†æˆæµ‹è¯•éœ€è¦å®Œæ•´æ•°æ®é›†ï¼Œè·³è¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ ESFormer é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 50)
    print("å¼€å§‹ ESFormer ç»„ä»¶æµ‹è¯• (ä¿®å¤ç‰ˆ)")
    print("=" * 50)
    
    tests = [
        ("WindowAttention", test_window_attention),
        ("SwinTransformerBlock", test_swin_transformer_block),
        ("PatchMerging", test_patch_merging),
        ("BasicLayer", test_basic_layer),
        ("SwinTransformerPlus", test_swin_transformer_plus),
        ("ESFormeré›†æˆ", test_esformer_integration),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        if test_func():
            passed += 1
    
    print("\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æžœ: {passed}/{total} é€šè¿‡")
    
    if passed >= total - 1:  # å…è®¸é›†æˆæµ‹è¯•è·³è¿‡
        print("ðŸŽ‰ æ ¸å¿ƒç»„ä»¶æµ‹è¯•é€šè¿‡! ESFormer åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return passed >= total - 1


if __name__ == "__main__":
    main()